{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "You are welcome to use this coding environment to train your agent for the project.  Follow the instructions below to get started!\n",
    "\n",
    "### 0. Get Prepared\n",
    "Follow these instructions if you are working locally (you want to setup a Jupyter Notebook locally).\n",
    " Otherwise skip to Section 1.\n",
    "\n",
    "#### 0.1 Setup the right Python version\n",
    "Use Python 3.6. You may want to either use a virtual env or setup a Conda environment as explained here:\n",
    "\n",
    "https://github.com/udacity/deep-reinforcement-learning#dependencies\n",
    "\n",
    "#### 0.2 Get the python dependencies requirements.txt\n",
    "Checkout the ./python folder of the Udacity DRLND repo:\n",
    "\n",
    "https://github.com/udacity/deep-reinforcement-learning\n",
    "\n",
    "It has a requirements.txt to ensure installing the necessary dependencies in the version specified by the course\n",
    "(do this only when you want to work locally). Otherwise use the reference notebook provided by the course which is pre setup properly.\n",
    "\n",
    "#### 0.3 Get the Pre-built Unity Environment\n",
    "The course requires you to use the pre-built environment.\n",
    "Get the right version for you from here:\n",
    "\n",
    "https://s3-us-west-1.amazonaws.com/udacity-drlnd\n",
    "\n",
    "For e.g. to get the NoVis version for Linux use this link\n",
    "https://s3-us-west-1.amazonaws.com/udacity-drlnd/P1/Banana/Banana_Linux_NoVis.zip\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "Run the next code cell to install a few packages.  This line will take a few minutes to run!\n",
    "\n",
    "Make sure you have the python folder with the right requirements.txt. See Section 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[31mERROR: Invalid requirement: './python'\r\n",
      "Hint: It looks like a path. File './python' does not exist.\u001B[0m\r\n",
      "\u001B[33mWARNING: You are using pip version 21.1.2; however, version 21.1.3 is available.\r\n",
      "You should consider upgrading via the '/home/q409893/work/repos/udacity/venv/bin/python -m pip install --upgrade pip' command.\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip -q install ./python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment is already saved in the Workspace and can be accessed at the file path provided below.  Please run the next code cell without making any changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "ss\n",
    "#IMPORTANT: The version with visualization never worked for me so I opted to use the NoVis (no visualization) version.\n",
    "env = UnityEnvironment(file_name=\"Banana_Linux_NoVis/Banana.x86_64\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Examine the State and Action Spaces\n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "print('Number of agents:', len(env_info.agents))\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "print('Number of actions:', action_size)\n",
    "\n",
    "# examine the state space \n",
    "state = env_info.vector_observations[0]\n",
    "print('States look like:', state)\n",
    "state_size = len(state)\n",
    "print('States have length:', state_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Take Random Actions in the Environment\n",
    "\n",
    "<span style=\"color:red\">WARNING: DON'T EXECUTE THIS CELL IF YOU WILL ACTUALLY USE THE ENV FOR TRAINING BELOW. The provided unity env is buggy and training simply fails. You would need to restart the Kernel.</span>.\n",
    "\n",
    "In the next code cell, you will learn how to use the Python API to control the agent and receive feedback from the environment.\n",
    "\n",
    "Note that **in this coding environment, you will not be able to watch the agent while it is training**, and you should set `train_mode=True` to restart the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"resetting environment\")\n",
    "env_info = env.reset(train_mode=False)[brain_name] # reset the environment\n",
    "state = env_info.vector_observations[0]            # get the current state\n",
    "print(\"getting initial state\")\n",
    "score = 0                                          # initialize the score\n",
    "index=0\n",
    "while True:\n",
    "    print(f\"step: {index}\", end='\\r')\n",
    "    action = np.random.randint(action_size)        # select an action\n",
    "    env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "    next_state = env_info.vector_observations[0]   # get the next state\n",
    "    reward = env_info.rewards[0]                   # get the reward\n",
    "    done = env_info.local_done[0]                  # see if episode has finished\n",
    "    score += reward                                # update the score\n",
    "    state = next_state                             # roll over the state to next time step\n",
    "    index += 1\n",
    "    if done:                                       # exit loop if episode finished\n",
    "        break\n",
    "    \n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training a Smart Agent\n",
    "\n",
    "Now it's your turn to train your own agent to solve the environment!  A few **important notes**:\n",
    "- When training the environment, set `train_mode=True`, so that the line for resetting the environment looks like the following:\n",
    "```python\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "```\n",
    "- If you use the Udacity Jupyter notebook, you will not be able to watch the agent while it is training.  However, **_after training the agent_**, you can download the saved model weights to watch the agent on your own machine!\n",
    "In that case make sure you don't use the NoVis environment locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "def dqn(agent,n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    \n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        state = env_info.vector_observations[0]\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            action = agent.act(state, eps)\n",
    "            env_info = env.step(action)[brain_name]        # send the action to the environment\n",
    "            next_state = env_info.vector_observations[0]   # get the next state\n",
    "            reward = env_info.rewards[0]                   # get the reward\n",
    "            done = env_info.local_done[0]                  # see if episode has finished\n",
    "            agent.step(state.astype(np.float32), action, reward, next_state.astype(np.float32), done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=13.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint_dropout_p{}_hiddenlayers{}.pth'.format(agent.qnetwork_local.drop_p,agent.qnetwork_local.hidden_layers_config))\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "def plot_scores(scores, axis_handle=None, label=\"\"):\n",
    "    # plot the scores\n",
    "    if axis_handle is None:\n",
    "        f, axis_handle = plt.subplots()\n",
    "    axis_handle.plot(np.arange(len(scores)), scores,label=label)\n",
    "    plt.ylabel('Score')\n",
    "    plt.xlabel('Episode #')\n",
    "    plt.legend()\n",
    "    return axis_handle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from dqn_agent_navigation_project import Agent\n",
    "#env = UnityEnvironment(file_name=\"/data/Banana_Linux_NoVis/Banana.x86_64\")\n",
    "#env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "agents_params=[\n",
    "    (0.0, [64,64]),\n",
    "    (0.3, [64,64]),\n",
    "    (0.0,[64,64,64]),\n",
    "    (0.0,[32,64,32]),\n",
    "    (0.0,[32,32,32,32,32])\n",
    "]\n",
    "\n",
    "axis_handle=None\n",
    "for agent_params in agents_params:\n",
    "    print(\"--------------\\nTrying out dropout_probab {} and hidden_layers {}\".format(agent_params[0],agent_params[1]))\n",
    "    agent = Agent(state_size=37, action_size=4, seed=0,drop_p=agent_params[0],hidden_layers_config=agent_params[1],\n",
    "                  duelling_networks=True,\n",
    "                  priority_experience=False)\n",
    "    scores = dqn(agent=agent)\n",
    "    axis_handle=plot_scores(scores,axis_handle,label=str(agent_params))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Shutting Down\n",
    "When finished, you can close the environment.\n",
    "\n",
    "<span style=\"color:red\">WARNING: Do this only if you are really done. You can't reopen the env unless you restart the Kernel due to buggy unity env :( </span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# env.close() # Ensuring the environment is not closed so that training can commence"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}